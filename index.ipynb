{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Sequence Models - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll learn to make use of **_Bidirectional Models_** to better classify sequences of text!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Understand and explain the basic architecture of a Bidirectional RNN. \n",
    "* Identify the types of problems Bidirectional approaches are best suited for. \n",
    "* Build and train Bidirectional RNN Models. \n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In this lab, we're going to use a **_Bidrectional LSTM Model_** to build a model that can identify toxic comments on social media. This dataset comes from the [Toxic Comment Classification Challenge on Kaggle](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) in partnership with Google and Jigsaw.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "From the \"Data\" section of the Kaggle competition linked above:\n",
    "\n",
    "\"You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n",
    "\n",
    "* toxic\n",
    "* severe_toxic\n",
    "* obscene\n",
    "* threat\n",
    "* insult\n",
    "* identity_hate\n",
    "\n",
    "You must create a model which predicts a probability of each type of toxicity for each comment.\"\n",
    "\n",
    "This tells us a couple things about the problem, which will affect the overall architecture of our model. Although this may technically be a multiclass classification problem, in practice, our model will treat each comment as 6 concurrent instances of binary classification. This is because a toxic comment can fall into one or more of the categories listed above--for example, a  comment may be toxic, obscene, threatening, and insulting, all at the same time. \n",
    "\n",
    "Run the cell below to import everything we'll need for this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "We'll start by loading in our training and testing data. You'll find the data stored inside of the file `data.zip` included in this repo. \n",
    "\n",
    "**_NOTE:_**  Before we can begin loading in the data using pandas, you'll first need to unzip the file. Go into the repo you've cloned and unzip the `data` folder into the same directory as this jupyter notebook now.\n",
    "\n",
    "Next, we'll use pandas to load in our training and testing data, and then downsample to only 20% of the training data, in the interest of training time. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Use pandas to read the training data from `data/train.csv`. Store this data in `train`.\n",
    "* Set `train` equal to `train.sample(frac=0.2)`, so that we'll only use 20% of the data to train our model. Otherwise, training this model could take several hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('data.zip', 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "train = train.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Next, we'll get the values for both our labels and the comments that will act as our training and testing data. We do this in order to get the data from pandas DataFrames to numpy arrays. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create an array called `list_classes` that contains the following classes, in this order:\n",
    "    * `'toxic'`\n",
    "    * `'severe_toxic'`\n",
    "    * `'obscene'`\n",
    "    * `'threat'`\n",
    "    * `'insult'`\n",
    "    * `'identity_hate'`\n",
    "* Store the `.values` of the DataFrame that is returned by using `list_classes` to slice the label columns from `train` (slice `list_classes` from train, and then chain it with `.values`). Store this in `y`.\n",
    "* Store the `.values` of `train['comment_text]` in `list_sentences_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y = train[list_classes].values\n",
    "list_sentences_train = train.comment_text.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the data dictionary for this Kaggle competition, there are no missing values in either the train or the test set. However, let's quickly double check, just to be sure!\n",
    "\n",
    "Run the cell below to see if there are any missing values in either the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check that there are no missing values in either training set\n",
    "train['comment_text'].isna().any() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing The Data\n",
    "\n",
    "Next, We'll need to preprocess our data. We've already learned how to do most of this by working with NLTK--however, keras also contains some excellent preprocessing packages to help prepare text data.  Since we'll be feeding this data right into a model built with keras, this has the added benefit of ensuring that our data will be in a format that our model will be able to work with, meaning that we can avoid the weird bugs that sometimes occur when working with multiple different 3rd party libraries at the same time. \n",
    "\n",
    "Our preprocessing steps are:\n",
    "\n",
    "1. **_Tokenize_** the data. \n",
    "2. Turn the tokenized text into **_Sequences_**\n",
    "3. **_Pad_** the sequences so they're all the same length. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create a `Tokenizer`, which can be found inside the `text` module we imported at the top of the lab. Set the `num_words` parameter to `20000`, so that our model only uses the 20000 most common words. \n",
    "* Convert `list_sentences_train` to a python list, and then pass it in to our tokenizer's `.fit_on_texts()` method. \n",
    "* Call the tokenizer's `texts_to_sequences()` method on `list_sentences_train` and store the result returned in `list_tokenized_train`\n",
    "* Use the `sequence` module's `pad_sequences()` method and pass in `list_tokenized_train`, as well as the parameter `maxlen=100`. Store the result returned in `X_t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyternotify in /Users/steeznation/anaconda3/lib/python3.7/site-packages (0.1.15)\n",
      "Requirement already satisfied: jupyter in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from jupyternotify) (1.0.0)\n",
      "Requirement already satisfied: ipython in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from jupyternotify) (7.2.0)\n",
      "Requirement already satisfied: qtconsole in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from jupyter->jupyternotify) (4.4.3)\n",
      "Requirement already satisfied: ipywidgets in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from jupyter->jupyternotify) (7.4.2)\n",
      "Requirement already satisfied: ipykernel in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from jupyter->jupyternotify) (5.1.0)\n",
      "Requirement already satisfied: jupyter-console in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from jupyter->jupyternotify) (6.0.0)\n",
      "Requirement already satisfied: notebook in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from jupyter->jupyternotify) (5.7.4)\n",
      "Requirement already satisfied: nbconvert in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from jupyter->jupyternotify) (5.4.0)\n",
      "Requirement already satisfied: pygments in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from ipython->jupyternotify) (2.3.1)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from ipython->jupyternotify) (2.0.7)\n",
      "Requirement already satisfied: jedi>=0.10 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from ipython->jupyternotify) (0.13.2)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from ipython->jupyternotify) (0.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from ipython->jupyternotify) (40.6.3)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from ipython->jupyternotify) (4.3.2)\n",
      "Requirement already satisfied: backcall in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from ipython->jupyternotify) (0.1.0)\n",
      "Requirement already satisfied: decorator in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from ipython->jupyternotify) (4.3.0)\n",
      "Requirement already satisfied: pickleshare in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from ipython->jupyternotify) (0.7.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from ipython->jupyternotify) (4.6.0)\n",
      "Requirement already satisfied: ipython_genutils in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from qtconsole->jupyter->jupyternotify) (0.2.0)\n",
      "Requirement already satisfied: jupyter_core in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from qtconsole->jupyter->jupyternotify) (4.4.0)\n",
      "Requirement already satisfied: jupyter_client>=4.1 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from qtconsole->jupyter->jupyternotify) (5.2.4)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from ipywidgets->jupyter->jupyternotify) (3.4.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from ipywidgets->jupyter->jupyternotify) (4.4.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from ipykernel->jupyter->jupyternotify) (5.1.1)\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from notebook->jupyter->jupyternotify) (17.1.2)\n",
      "Requirement already satisfied: prometheus-client in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from notebook->jupyter->jupyternotify) (0.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from notebook->jupyter->jupyternotify) (0.8.1)\n",
      "Requirement already satisfied: jinja2 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from notebook->jupyter->jupyternotify) (2.10)\n",
      "Requirement already satisfied: Send2Trash in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from notebook->jupyter->jupyternotify) (1.5.0)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from nbconvert->jupyter->jupyternotify) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from nbconvert->jupyter->jupyternotify) (0.2.3)\n",
      "Requirement already satisfied: bleach in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from nbconvert->jupyter->jupyternotify) (3.0.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from nbconvert->jupyter->jupyternotify) (1.4.2)\n",
      "Requirement already satisfied: testpath in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from nbconvert->jupyter->jupyternotify) (0.4.2)\n",
      "Requirement already satisfied: defusedxml in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from nbconvert->jupyter->jupyternotify) (0.5.0)\n",
      "Requirement already satisfied: wcwidth in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyternotify) (0.1.7)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyternotify) (1.12.0)\n",
      "Requirement already satisfied: parso>=0.3.0 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from jedi>=0.10->ipython->jupyternotify) (0.3.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython->jupyternotify) (0.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from jupyter_client>=4.1->qtconsole->jupyter->jupyternotify) (2.7.5)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets->jupyter->jupyternotify) (2.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from jinja2->notebook->jupyter->jupyternotify) (1.1.0)\n",
      "Requirement already satisfied: webencodings in /Users/steeznation/anaconda3/lib/python3.7/site-packages (from bleach->nbconvert->jupyter->jupyternotify) (0.5.1)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install jupyternotify\n",
    "import jupyternotify\n",
    "ip = get_ipython()\n",
    "ip.register_magics(jupyternotify.JupyterNotifyMagics)\n",
    "## Run %%notify to create notification for completed cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"2699236c-1703-4b23-ace4-7830e894b8a6\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"2699236c-1703-4b23-ace4-7830e894b8a6\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: This cell may take a little while to run!\n",
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=100)\n",
    "%notify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Model\n",
    "\n",
    "Now that we've loaded and preprocessed our data, we're ready to begin designing our model. By now, working with keras to create and compile a model will probably feel familiar to you. To keep things simple, we've left the name of each layer below. Your job will be to create each layer, and specify the previous layer that acts as it's input (which is why so many of the layers are called `x` below--you've probably noticed this simplifies the creation process by eliminating the need to keep track of which layer is which at any given point). \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Set the `embedding_size` to `128`\n",
    "* Create an `Input` layer that takes in data of `shape=(100,)`\n",
    "* Next, create an `Embedding` layer and pass in `20000` and `embedding_size` as parameters. Make sure to specify that the Embedding layer takes in the output of the input layer as its input by ending the line with `(input_)`\n",
    "* Create a `Bidirectional` layer. Inside this layer, pass in an `LSTM()`. The parameters for the LSTM should be `25`, and `return_sequences=True`. \n",
    "* Create a `GlobalMaxPool1D` Layer\n",
    "* Create a `Dropout` layer, and pass in `0.5` as a parameter. \n",
    "* Create a `Dense` layer with `50` neurons, and set the `activation` to `'relu'`\n",
    "* Create another `Dropout` layer, and pass in `0.5` as the parameter. \n",
    "* Create a `Dense` layer with `6` neurons, and set the `activation` to `'sigmoid'`. \n",
    "* Create a `Model` and set the `inputs` to `input_` and `outputs` to `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/steeznation/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/steeznation/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "input_ = Input(shape=(100,))\n",
    "x = Embedding(20000, embedding_size)(input_)\n",
    "x = Bidirectional(LSTM(25, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we've created our model, we still need to compile it.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Call `model.compile` and pass in the following parameters:\n",
    "    * `loss='binary_crossentropy'`\n",
    "    * `optimizer='adam'`\n",
    "    * `metrics=['accuracy']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the model we've created. In the cell below, call `model.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 50)           30800     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 2,593,656\n",
      "Trainable params: 2,593,656\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Some Checkpoints\n",
    "\n",
    "Training models like this can be tricky. Because of that, we'll make use of **_Checkpoints_** to help us periodically save our work in case things go wrong during training. \n",
    "\n",
    "We'll create two different types of checkpoints below:\n",
    "\n",
    "* A `ModelCheckpoint` that saves the best weights for our model at any given time inside an `hdf5` file. This way, if our model's performance starts to degrade at any point, we can always reload the weights from a snapshot of when it had the best possible performance. \n",
    "\n",
    "* An `EarlyStopping` checkpoint, which will stop the training early if the model goes for a certain number of epochs without any progress. \n",
    "\n",
    "For this lab, we'll only be training the model for a single epoch, so we don't actually need to use these checkpoints. However, on the job, models like this are often trained for days at a time. With training times that long, checkpoints are absolutely crucial to avoid losing days of work. There are few things more frustrating than seeing that you model was performing really well 2 days ago, but has since began to have performance degrade due to overfitting, and you have to start the training over because you forgot to set checkpoints!\n",
    "\n",
    "Run the cells below to create the checkpoints and store them in an array that we'll pass in during training. For more information on the checkpoints we've created, see the [Keras callbacks documentation](https://keras.io/callbacks/#earlystopping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = 'weights_base.best.hdf5'\n",
    "checkpoint = ModelCheckpoint(checkpoints_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now, we're ready to train our data. Because our model contains over 1.9 million trainable parameters, this will take a little while to train! \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Call `model.fit()` and pass in the following parameters:\n",
    "    * `X_t`\n",
    "    * `y`\n",
    "    * `batch_size=32`\n",
    "    * `epochs=1`\n",
    "    * `validation_split=0.1`\n",
    "    * `callbacks=callbacks`\n",
    "    \n",
    "**_NOTE:_** Running the cell below may take 15+ minutes, depending on your machine. Run it, then go get a coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/steeznation/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/steeznation/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 28722 samples, validate on 3192 samples\n",
      "Epoch 1/1\n",
      "28722/28722 [==============================] - 115s 4ms/step - loss: 0.1359 - acc: 0.9607 - val_loss: 0.0609 - val_acc: 0.9785\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06092, saving model to weights_base.best.hdf5\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"a1c9262e-c88e-4d9d-8dc3-e58ef2ca15cb\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"a1c9262e-c88e-4d9d-8dc3-e58ef2ca15cb\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=32, epochs=1, validation_split=0.1, callbacks=callbacks)\n",
    "%notify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of over 97.8% when trained on only 20% of the data--this is excellent! If you train on the entire training set, you'll see that we achieve over 98% accuracy after only 1 epoch of training. It's safe to say our model works pretty well!\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, we incorporated everything we've learned about sequence models and embedding layers to build a Bidirectional LSTM Network to successfully classify toxic comments from wikipedia!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
